{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验要求\n",
    "### 截止日期：12月15日\n",
    "作业的提交格式参考之前的说明，提交到18329300691@163.com\n",
    "### 基本要求\n",
    "a)\t基于 Watermelon-train1数据集（只有离散属性），构造ID3决策树；\n",
    "b)\t基于构造的 ID3 决策树，对数据集 Watermelon-test1进行预测，输出分类精度；\n",
    "### 中级要求\n",
    "a)  对数据集Watermelon-train2，构造C4.5或者CART决策树，要求可以处理连续型属性；\n",
    "b)\t对测试集Watermelon-test2进行预测，输出分类精度；\n",
    "### 高级要求\n",
    "使用任意的剪枝算法对构造的决策树（基本要求和中级要求构造的树）进行剪枝，观察测试集合的分类精度是否有提升，给出分析过程。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是决策树\n",
    "<img src=\"https://s2.loli.net/2022/10/23/yTpSLmgFWOh4Y5d.png\" style=\"zoom:66%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的划分\n",
    "- 决策树主要分为三种：\n",
    "\tID3，C4.5和CART，它们分别对应的**特征选择准则**是信息增益（ID3），信息增益比（C4.5）和基尼指数（CART）。\n",
    "\t它们决定当前选择哪个特征进行数据划分，使得样本在当下能够被最大程度的划分。\n",
    "- 对于离散变量，选定**属性**分类即可；\n",
    "- 对于连续变量，需要选定**划分点**。\n",
    "- CART和C4.5支持数据特征为**连续分布**时的处理，能够完成对连续属性的离散化处理，主要通过二元切分的方式来处理连续型变量，这个分裂点的选择原则是使得划分后的子树中的“混乱程度”降低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3算法\n",
    "- ID3算法的核⼼思想应用信息增益准则作为标准,介绍信息增益之前首先介绍一下信息熵和条件熵： \n",
    "- 熵（entropy）概念：\n",
    "\t    1948年，香农提出了“信息熵”的概念。在信息论与概率统计中，熵是表示随机变量不确定性的量。X是⼀个取值为有限个的离散随机变量，\n",
    "$$ H(X)=-\\sum_{i=1}^{n} p\\left(x_{i}\\right) \\log p\\left(x_{i}\\right)$$ \n",
    "$𝐻(𝑋)$就被称作随机变量𝑋的熵，它表示随机变量不确定的度量。熵取值越大，随机变量不确定性越大。当随机变量为均匀分布时，熵最大。当某一状态概率取值为1时，熵的值为零。\n",
    "\n",
    "### ID3算法-条件熵和信息增益\n",
    "- 条件熵 $𝐻(𝑌∣𝑋)$ ：\n",
    "\t表示在已知随机变量𝑋的条件下随机变量𝑌的不确定性，定义为给定𝑋条件下𝑌的条件概率分布的熵对𝑋的数学期望:\n",
    "$$H(Y \\mid X)=\\sum_{x} p(x) H(Y \\mid X=x) =-\\sum_{x} p(x) \\sum_{y} p(y \\mid x) \\log p(y \\mid x)$$\n",
    "\n",
    "- 特征𝐴对数据集𝐷的信息增益就是熵$𝐻(𝐷)$与条件熵$𝐻(𝐷|𝐴)$之差:\n",
    "$$𝐻(𝐷)−𝐻(𝐷∣𝐴)$$\n",
    "\n",
    "\t表示已知特征𝐴的信息而使得数据集𝐷的信息不确定减少的程度。信息增益越大的特征代表其具有更强的分类能力，所以我们就要**选择能够使数据的不确定程度减少最多的特征**，也就是信息增益最大的特征。\n",
    "\n",
    "### ID3算法-停止条件\n",
    "- 决策树的生成:\n",
    "\n",
    "\t从根节点开始，计算所有可能特征的信息增益，选择信息增益最大的特征作为划分该节点的特征，根据该特征的不同取值建立子节点；\n",
    "\t在对子节点递归地调用以上方法，直到达到停止条件，得到⼀个决策树。\n",
    "    \n",
    "    \n",
    "- 迭代停止条件：\n",
    "  1. 当前结点所有样本都属于同⼀类别；\n",
    "  2. 当前结点的所有属性值都相同，没有剩余属性可用来进一步划分样本；\n",
    "  3. 达到最大树深；\n",
    "  4. 达到叶子结点的最小样本数；\n",
    "\n",
    "### ID3算法举例\n",
    "\n",
    "<img src=\"https://s2.loli.net/2022/10/23/p7gSQeYGnoBCd2i.png\" style=\"zoom:64%\" />\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\operatorname{Info}^{\\text {In }}(D)=-\\frac{9}{14} \\log _{2}\\left(\\frac{9}{14}\\right)-\\frac{5}{14} \\log _{2}\\left(\\frac{5}{14}\\right)=0.940 \\\\\n",
    "\\operatorname{Infoage~}(D)=\\frac{5}{14} \\times\\left(-\\frac{2}{5} \\times \\log _{2} \\frac{2}{5}-\\frac{3}{5} \\times \\log _{2} \\frac{3}{5}\\right)+\\frac{4}{14} \\times\\left(-\\frac{4}{4} \\times \\log _{2} \\frac{4}{4}-\\frac{0}{4} \\times \\log _{2} \\frac{0}{4}\\right) \n",
    "+\\frac{5}{14} \\times\\left(-\\frac{2}{5} \\times \\log _{2} \\frac{2}{5}-\\frac{3}{5} \\times \\log _{2} \\frac{3}{5}\\right)=0.694 \\\\\n",
    "\\text { Gain }(\\text { age })=\\operatorname{Info}(D)-\\operatorname{InfO}_{\\text {age }}(D) =0.940-0.694=0.246\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "<img src=\"https://s2.loli.net/2022/10/23/1zAnHWKRgQ9FaJV.png\" style=\"zoom:72%\" />\n",
    "类似地，\n",
    "Gain(income)=0.029    \n",
    "Gain(student)=0.151    \n",
    "Gain(credit_rating)=0.048\n",
    "\n",
    "所以，选择age作为第一个根节点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5算法\n",
    "- C4.5算法与ID3算法相似，其对ID3算法进行了改进。\n",
    "- 信息增益作为划分准则存在的问题：\n",
    "\n",
    "     信息增益偏向于选择取值较多的特征进行划分。⽐如学号这个特征，每个学生都有一个不同的学号，如果根据学号对样本进行分类，则每个学生都属于不同的类别，这样是没有意义的。而C4.5在生成过程中，用**信息增益比**来选择特征，可以校正这个问题。\n",
    "     \n",
    "- 特点\n",
    "  - 能够完成对连续属性的离散化处理\n",
    "  - 能够对不完整数据进行处理\n",
    "  - 需要对数据集进行多次的顺序扫描和排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART算法\n",
    "- ID3和C4.5虽然在对训练样本集的学习中可以尽可能多的挖掘信息，但其生成的决策树分支较大，规模较大。为了简化决策树的规模，提高生成决策树的效率，就出现了根据**基尼指数**来选择的CART； \n",
    "- 对于给定的样本集合 ，其基尼指数为： $$ {Gini}(D)=1-\\sum_{k=1}^{K}\\left(\\frac{\\left|C_{k}\\right|}{|D|}\\right)^{2} $$\n",
    "   其中$𝐶_𝑘$是𝐷中属于第𝑘类的样本子集，K是类的个数。\n",
    "- 基尼系数的性质与信息熵一样：\n",
    "   度量随机变量的不确定度的大小；基尼指数越⼩表示数据的纯度越高，反之其值越大，样本集合的不确定性也就越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的剪枝\n",
    "- 决策树很容易出现**过拟合现象**。原因在于学习时完全考虑的是如何提⾼对训练数据的正确分类从⽽构建出过于复杂的决策树。\n",
    "- 解决这个问题的方法称为**剪枝**，即对已生成的树进行简化。具体地，就是从已生成的树上裁剪掉⼀些子树或叶节点，并将其根节点或父节点作为新的叶节点。 \n",
    "- 决策树的剪枝基本策略有**预剪枝 (Pre-Pruning)** 和 **后剪枝 (Post-Pruning)**\n",
    "   - **预剪枝**：是根据⼀些原则**极早的停止树增长**，如树的深度达到用户所要的深度、节点中样本个数少于用户指定个数、不纯度指标下降的幅度小于用户指定的幅度等。 \n",
    "   - **后剪枝**：是通过在完全生长的树上剪去分枝实现的，通过删除节点的分支来剪去树节点。是在生成决策树之后**自底向上**的对树中所有的非叶结点进⾏逐一考察 。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
