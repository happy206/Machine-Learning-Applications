{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "775f8228-fc8e-4348-bb92-237bfabc36b5",
   "metadata": {},
   "source": [
    "# 实验报告\n",
    "<font size=4>\n",
    "    \n",
    "+ **姓名：于成俊**\n",
    "+ **学号：2112066**\n",
    "+ **专业：密码科学与技术**\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0c06c6-2402-46e6-af36-53ec2be0c49d",
   "metadata": {},
   "source": [
    "## 实验要求\n",
    "    \n",
    "<font size=4>\n",
    "    \n",
    "\n",
    "+ 基本要求：\n",
    "    + a) 基于 Watermelon-train1数据集（只有离散属性），构造ID3决策树；\n",
    "    + b) 基于构造的 ID3 决策树，对数据集 Watermelon-test1进行预测，输出分类精度；\n",
    "+ 中级要求：\n",
    "    + a) 对数据集Watermelon-train2，构造C4.5或者CART决策树，要求可以处理连续型属性；\n",
    "    + b) 对测试集Watermelon-test2进行预测，输出分类精度；\n",
    "+ 高级要求：\n",
    "    + 使用任意的剪枝算法对构造的决策树（基本要求和中级要求构造的树）进行剪枝，观察测试集合的分类精度是否有提升，给出分析过程。\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f11178-fc7d-431f-97ca-50caf28012ff",
   "metadata": {},
   "source": [
    "## 实验流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be1b465-607f-4ee6-8b68-70873b73d50d",
   "metadata": {},
   "source": [
    "首先导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23633132-5456-4843-b22d-4913ac851f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38af31d4-801e-40dd-b191-fd24583fb1ba",
   "metadata": {},
   "source": [
    "编写读取数据集函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5138cc9-30f2-4afd-b3e4-8eaa0c1ebc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "def read_dataset(filename):\n",
    "    # 由于文件是‘GBK’编码，所以采用GBK的方式打开文件\n",
    "    with open(filename, 'r', encoding='GBK') as f:\n",
    "        reader = csv.reader(f)\n",
    "        return list(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd9a0b8-7005-4b7a-bc8a-bae3bf82c268",
   "metadata": {},
   "source": [
    "### 基本要求\n",
    "#### 构造ID3决策树："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caed716-093c-437a-864f-94b2f9640675",
   "metadata": {},
   "source": [
    "**构造ID3决策树类:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd780b64-1866-4f3d-b60a-3e8f8b77a6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3:\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "\n",
    "    class Node:\n",
    "        def __init__(self, attribute=None, label=None):\n",
    "            self.attribute = attribute\n",
    "            self.label = label\n",
    "            self.children = {}\n",
    "\n",
    "    def fit(self, data, labels):\n",
    "        # 使用 ID3 算法构建决策树，得到根节点\n",
    "        self.root = self._id3(data, labels, list(range(len(data[0]))))\n",
    "\n",
    "    def _id3(self, data, labels, attributes):\n",
    "        #  将labels 转换为集合（set）,从而计算不同标签的数量\n",
    "        if len(set(labels)) == 1:\n",
    "            # 如果只有一个标签，说明所有数据点都属于同一标签，则创建一个该标签的叶子节点\n",
    "            return self.Node(label=labels[0])\n",
    "        # 如果没有可用的属性（特征），则同样创建一个叶子节点，节点的标签为出现次数最多的标签。\n",
    "        if len(attributes) == 0:\n",
    "            return self.Node(label=max(set(labels), key=labels.count))\n",
    "        # 选择具有最大信息增益的属性\n",
    "        best_attr = max(attributes, key=lambda attr: self._gain(data, labels, attr))\n",
    "        # 创建一个具有最大信息增益的属性的节点\n",
    "        node = self.Node(attribute=best_attr)\n",
    "        # 遍历best_attr的所有可能取值。\n",
    "        for value in set(row[best_attr] for row in data):\n",
    "            # 根据best_attr的取值将数据和标签进行划分，创建子集。\n",
    "            child_data = [row for row, l in zip(data, labels) if row[best_attr] == value]\n",
    "            child_labels = [l for row, l in zip(data, labels) if row[best_attr] == value]\n",
    "            # 如果子集为空，说明所有数据都具有相同的属性值，创建一个叶子节点，节点的标签为目标变量中出现次数最多的标签\n",
    "            if not child_data:\n",
    "                # 将子节点添加到当前节点的子节点中。\n",
    "                node.children[value] = self.Node(label=max(set(labels), key=labels.count))\n",
    "            else:\n",
    "                # 递归调用 _id3 方法，继续构建决策树。\n",
    "                node.children[value] = self._id3(child_data, child_labels, [attr for attr in attributes if attr != best_attr])\n",
    "\n",
    "        return node\n",
    "\n",
    "    # 计算信息增益\n",
    "    def _gain(self, data, labels, attr):\n",
    "        # 计算整体数据集的熵\n",
    "        entropy = self._entropy(labels)\n",
    "        # 获取属性列的值\n",
    "        values = [row[attr] for row in data]\n",
    "        # 将数据集按照属性值分割成不同的分区\n",
    "        partitions = [[t for v, t in zip(values, labels) if v == value] for value in set(values)]\n",
    "        # 计算信息增益\n",
    "        return entropy - sum(len(part) / len(labels) * self._entropy(part) for part in partitions)\n",
    "\n",
    "    def _entropy(self, labels):\n",
    "        # 计算每个标签在标签集中的出现次数\n",
    "        counts = [labels.count(value) for value in set(labels)]\n",
    "        # 计算信息熵\n",
    "        return -sum(count / len(labels) * math.log2(count / len(labels)) for count in counts)\n",
    "\n",
    "    def predict(self, data):\n",
    "        # 返回一个列表，包含对每个数据点的预测结果\n",
    "        return [self._predict(row, self.root) for row in data]\n",
    "\n",
    "    def _predict(self, row, node):\n",
    "        # 如果节点是叶子节点，直接返回叶子节点的标签值\n",
    "        if node.label is not None:\n",
    "            return node.label\n",
    "        # 如果节点不是叶子节点，继续递归预测\n",
    "        return self._predict(row, node.children[row[node.attribute]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5e06b3-ae4b-418c-b89a-33496f1cc3ea",
   "metadata": {},
   "source": [
    "**使用ID3决策树进行训练和预测：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25909208-a0bd-4f06-a5c8-99b95dbb54fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用ID3决策树进行训练和预测\n",
    "def ID3_train_predict():\n",
    "    # 读取训练集和测试集数据\n",
    "    train_data = read_dataset('Watermelon-train1.csv')\n",
    "    test_data = read_dataset('Watermelon-test1.csv')\n",
    "\n",
    "    # 创建 ID3 决策树实例\n",
    "    id3 = ID3()\n",
    "\n",
    "    # 使用训练集训练 ID3 决策树\n",
    "    id3.fit([row[1:-1] for row in train_data], [row[-1] for row in train_data])\n",
    "\n",
    "    # 对测试集进行预测\n",
    "    predictions = id3.predict([row[1:-1] for row in test_data])\n",
    "\n",
    "    # 计算分类精度\n",
    "    accuracy = sum(p == t[-1] for p, t in zip(predictions, test_data)) / len(test_data)\n",
    "\n",
    "    # 打印分类精度\n",
    "    print(f'分类精度: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8812541d-0389-4707-8b63-bf30d607a3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类精度: 0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "ID3_train_predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660ca514-a8fd-4952-98b7-ab04a316c2ba",
   "metadata": {},
   "source": [
    "### 中级要求\n",
    "#### 构造CART决策树："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eead96-9f63-465f-a0e9-f68c89482f06",
   "metadata": {},
   "source": [
    "**构造CART决策树类:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7fc93cd-cf55-44c9-adb9-d420d902dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CART:\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "\n",
    "    class Node:\n",
    "        def __init__(self, attribute=None, threshold=None, label=None, left=None, right=None):\n",
    "            self.attribute = attribute\n",
    "            self.threshold = threshold\n",
    "            self.label = label\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "\n",
    "    def fit(self, data, labels):\n",
    "        # 使用 CART 决策树算法构建决策树，将根节点保存在 self.root 中\n",
    "        self.root = self._cart(data, labels, list(range(len(data[0]))))\n",
    "\n",
    "    def _cart(self, data, labels, attributes):\n",
    "        # 如果标签集唯一，创建叶子节点并返回\n",
    "        if len(set(labels)) == 1:\n",
    "            return self.Node(label=labels[0])\n",
    "\n",
    "        # 如果没有可用的属性，创建叶子节点并返回，标签为目标变量中最频繁的值\n",
    "        if len(attributes) == 0:\n",
    "            return self.Node(label=max(set(labels), key=labels.count))\n",
    "\n",
    "        # 寻找最佳分裂属性和阈值\n",
    "        best_attr, best_threshold = self._best_split(data, labels, attributes)\n",
    "        node = self.Node(attribute=best_attr, threshold=best_threshold)\n",
    "\n",
    "        # 根据最佳分裂属性和阈值分割数据集\n",
    "        left_data, left_labels, right_data, right_labels = self._split_data(data, labels, best_attr, best_threshold)\n",
    "\n",
    "        # 如果左子集为空，创建叶子节点并返回，标签为左子集中最频繁的值\n",
    "        if not left_data:\n",
    "            node.label = max(set(labels), key=labels.count)\n",
    "        else:\n",
    "            # 递归构建左子树和右子树\n",
    "            node.left = self._cart(left_data, left_labels, [attr for attr in attributes if attr != best_attr])\n",
    "            node.right = self._cart(right_data, right_labels, [attr for attr in attributes if attr != best_attr])\n",
    "\n",
    "        return node\n",
    "\n",
    "    def _best_split(self, data, labels, attributes):\n",
    "        # 初始化最佳分裂属性、最佳阈值和最小的基尼指数\n",
    "        best_attr, best_threshold, min_gini = None, None, float('inf')\n",
    "\n",
    "        # 遍历所有属性\n",
    "        for attr in attributes:\n",
    "            # 获取当前属性的所有取值\n",
    "            values = [row[attr] for row in data]\n",
    "\n",
    "            # 遍历当前属性的每个唯一取值，将数据集分割为左右两部分\n",
    "            for value in set(values):\n",
    "                left_labels = [t for v, t in zip(values, labels) if v < value]\n",
    "                right_labels = [t for v, t in zip(values, labels) if v >= value]\n",
    "\n",
    "                # 计算基尼指数\n",
    "                gini = self._gini(left_labels) * len(left_labels) / len(labels) + self._gini(right_labels) * len(\n",
    "                    right_labels) / len(labels)\n",
    "\n",
    "                # 更新最小基尼指数及相应的属性和阈值\n",
    "                if gini < min_gini:\n",
    "                    best_attr, best_threshold, min_gini = attr, value, gini\n",
    "\n",
    "        # 返回最佳分裂属性和阈值\n",
    "        return best_attr, best_threshold\n",
    "\n",
    "    def _split_data(self, data, labels, attr, threshold):\n",
    "        # 初始化左右两部分的数据和标签集\n",
    "        left_data, left_labels, right_data, right_labels = [], [], [], []\n",
    "\n",
    "        # 遍历数据集中的每一行及其对应的目标变量\n",
    "        for row, l in zip(data, labels):\n",
    "            # 根据指定的属性和阈值进行分割\n",
    "            if row[attr] < threshold:\n",
    "                # 如果当前样本的指定属性值小于阈值，加入左侧部分\n",
    "                left_data.append(row)\n",
    "                left_labels.append(l)\n",
    "            else:\n",
    "                # 如果当前样本的指定属性值大于等于阈值，加入右侧部分\n",
    "                right_data.append(row)\n",
    "                right_labels.append(l)\n",
    "\n",
    "        # 返回左右两部分的数据和标签集\n",
    "        return left_data, left_labels, right_data, right_labels\n",
    "\n",
    "    def _gini(self, labels):\n",
    "        # 初始化基尼指数\n",
    "        gini = 1 - sum((labels.count(value) / len(labels)) ** 2 for value in set(labels))\n",
    "        return gini\n",
    "\n",
    "    def predict(self, data):\n",
    "        # 返回一个列表，其中每个元素是对应样本的预测值\n",
    "        return [self._predict(row, self.root) for row in data]\n",
    "\n",
    "    def _predict(self, row, node):\n",
    "        # 如果当前节点是叶子节点，直接返回节点的标签作为预测值\n",
    "        if node.label is not None:\n",
    "            return node.label\n",
    "        # 如果当前节点不是叶子节点，根据节点的划分条件继续向下遍历\n",
    "        if row[node.attribute] < node.threshold:\n",
    "            # 如果样本的指定属性值小于节点的阈值，则递归预测左子树\n",
    "            return self._predict(row, node.left)\n",
    "        # 如果样本的指定属性值大于等于节点的阈值，则递归预测右子树\n",
    "        return self._predict(row, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f2138c-00bf-497e-9062-8d1014cf5d13",
   "metadata": {},
   "source": [
    "**使用CART决策树进行训练和预测：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb0dd445-db9b-44ec-8a5d-8171f711f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CRAT_train_predict():\n",
    "    # 读取训练集和测试集数据\n",
    "    train_data = read_dataset('Watermelon-train2.csv')\n",
    "    test_data = read_dataset('Watermelon-test2.csv')\n",
    "\n",
    "    # 创建 CART 决策树实例\n",
    "    cart = CART()\n",
    "\n",
    "    # 使用训练集训练 CART 决策树\n",
    "    cart.fit([row[1:-1] for row in train_data], [row[-1] for row in train_data])\n",
    "\n",
    "    # 对测试集进行预测\n",
    "    predictions = cart.predict([row[1:-1] for row in test_data])\n",
    "\n",
    "    # 计算分类精度\n",
    "    accuracy = sum(p == t[-1] for p, t in zip(predictions, test_data)) / len(test_data)\n",
    "\n",
    "    # 打印分类精度\n",
    "    print(f'分类精度: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aa8a4a8-0e2e-4e56-b226-bd47911011fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类精度: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "CRAT_train_predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec4097-863c-416f-a9e5-c2ded3e4dd8c",
   "metadata": {},
   "source": [
    "### 高级要求"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df312c-c8f3-496b-b513-e7c80ca6e01e",
   "metadata": {},
   "source": [
    "首先对ID3树进行剪枝，剪枝的策略为如果所有子节点都是叶子节点且它们的标签相同，就将当前节点替换为叶子节点。这个剪枝算法在决策树构建完毕后进行，以提高模型的泛化能力。这种策略的核心思想是当一个节点的所有子节点都是叶子节点且它们的标签相同时，可以认为这个节点对训练数据进行了过度拟合。因此，\n",
    "将这个节点替换为一个叶子节点。其中，剪枝的ID3类比普通的ID3类多了一个prune函数，并且在fit函数的最后调用这个函数，其他函数与普通的ID3类完全相同。为了方便判断一个节点是否是叶子节点，还在Node类中加了一个判断函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374c7ea9-a498-44c8-9073-0dc10e7af993",
   "metadata": {},
   "source": [
    "**剪枝的ID3树如下：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "387ebdde-d797-41b2-824b-d9a1901c6651",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3_Pruning:\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "\n",
    "    class Node:\n",
    "        def __init__(self, attribute=None, label=None):\n",
    "            self.attribute = attribute\n",
    "            self.label = label\n",
    "            self.children = {}\n",
    "        # 判断节点是否为叶子节点\n",
    "        def is_leaf(self):\n",
    "            return not bool(self.children)\n",
    "\n",
    "    def prune(self, node):\n",
    "        # 如果所有的子节点都是叶子节点\n",
    "        if all(child.is_leaf() for child in node.children.values()):\n",
    "            # 获取所有子节点的标签\n",
    "            labels = [child.label for child in node.children.values()]\n",
    "            # 如果所有子节点的标签都相同，将当前节点替换为叶子节点\n",
    "            if len(set(labels)) == 1:\n",
    "                return self.Node(label=labels[0])\n",
    "        # 递归地对每个子节点进行剪枝\n",
    "        for child in node.children.values():\n",
    "            self.prune(child)\n",
    "        # 返回当前节点\n",
    "        return node\n",
    "\n",
    "    def fit(self, data, labels):\n",
    "        # 使用 ID3 算法构建决策树，得到根节点\n",
    "        self.root = self._id3(data, labels, list(range(len(data[0]))))\n",
    "        self.root = self.prune(self.root)  # 在训练结束后进行剪枝\n",
    "\n",
    "    def _id3(self, data, labels, attributes):\n",
    "        #  将labels 转换为集合（set）,从而计算不同标签的数量\n",
    "        if len(set(labels)) == 1:\n",
    "            # 如果只有一个标签，说明所有数据点都属于同一标签，则创建一个该标签的叶子节点\n",
    "            return self.Node(label=labels[0])\n",
    "        # 如果没有可用的属性（特征），则同样创建一个叶子节点，节点的标签为出现次数最多的标签。\n",
    "        if len(attributes) == 0:\n",
    "            return self.Node(label=max(set(labels), key=labels.count))\n",
    "        # 选择具有最大信息增益的属性\n",
    "        best_attr = max(attributes, key=lambda attr: self._gain(data, labels, attr))\n",
    "        # 创建一个具有最大信息增益的属性的节点\n",
    "        node = self.Node(attribute=best_attr)\n",
    "        # 遍历best_attr的所有可能取值。\n",
    "        for value in set(row[best_attr] for row in data):\n",
    "            # 根据best_attr的取值将数据和标签进行划分，创建子集。\n",
    "            child_data = [row for row, l in zip(data, labels) if row[best_attr] == value]\n",
    "            child_labels = [l for row, l in zip(data, labels) if row[best_attr] == value]\n",
    "            # 如果子集为空，说明所有数据都具有相同的属性值，创建一个叶子节点，节点的标签为目标变量中出现次数最多的标签\n",
    "            if not child_data:\n",
    "                # 将子节点添加到当前节点的子节点中。\n",
    "                node.children[value] = self.Node(label=max(set(labels), key=labels.count))\n",
    "            else:\n",
    "                # 递归调用 _id3 方法，继续构建决策树。\n",
    "                node.children[value] = self._id3(child_data, child_labels,\n",
    "                                                 [attr for attr in attributes if attr != best_attr])\n",
    "\n",
    "        return node\n",
    "\n",
    "    def _gain(self, data, labels, attr):\n",
    "        # 计算整体数据集的熵\n",
    "        entropy = self._entropy(labels)\n",
    "        # 获取属性列的值\n",
    "        values = [row[attr] for row in data]\n",
    "        # 将数据集按照属性值分割成不同的分区\n",
    "        partitions = [[l for v, l in zip(values, labels) if v == value] for value in set(values)]\n",
    "        # 计算信息增益\n",
    "        return entropy - sum(len(part) / len(labels) * self._entropy(part) for part in partitions)\n",
    "\n",
    "    def _entropy(self, labels):\n",
    "        # 计算每个标签在标签集中的出现次数\n",
    "        counts = [labels.count(value) for value in set(labels)]\n",
    "        # 计算信息熵\n",
    "        return -sum(count / len(labels) * math.log2(count / len(labels)) for count in counts)\n",
    "\n",
    "    def predict(self, data):\n",
    "        # 返回一个列表，包含对每个数据点的预测结果\n",
    "        return [self._predict(row, self.root) for row in data]\n",
    "\n",
    "    def _predict(self, row, node):\n",
    "        # 如果节点是叶子节点，直接返回叶子节点的标签值\n",
    "        if node.label is not None:\n",
    "            return node.label\n",
    "        # 如果节点不是叶子节点，继续递归预测\n",
    "        return self._predict(row, node.children[row[node.attribute]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b23cf5-01b6-4c18-94be-ae8ba2a6e1b5",
   "metadata": {},
   "source": [
    "**使用可以剪枝的ID3决策树进行训练和预测：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53312f0e-9a45-4149-88e4-559c96da2b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用可以剪枝的ID3决策树进行训练和预测\n",
    "def ID3_Pruning_train_predict():\n",
    "    # 读取训练集和测试集数据\n",
    "    train_data = read_dataset('Watermelon-train1.csv')\n",
    "    test_data = read_dataset('Watermelon-test1.csv')\n",
    "\n",
    "    # 创建 ID3 决策树实例\n",
    "    id3_Pruning = ID3_Pruning()\n",
    "\n",
    "    # 使用训练集训练 ID3 决策树\n",
    "    id3_Pruning.fit([row[1:-1] for row in train_data], [row[-1] for row in train_data])\n",
    "\n",
    "    # 对测试集进行预测\n",
    "    predictions = id3_Pruning.predict([row[1:-1] for row in test_data])\n",
    "\n",
    "    # 计算分类精度\n",
    "    accuracy = sum(p == t[-1] for p, t in zip(predictions, test_data)) / len(test_data)\n",
    "\n",
    "    # 打印分类精度\n",
    "    print(f'分类精度: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c22d227-0959-4bb2-83cb-f0ac5928569d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类精度: 0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "ID3_Pruning_train_predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7fdcfe-8bca-45a7-be7d-7e36be1aca1a",
   "metadata": {},
   "source": [
    "对CART树的剪枝策略与ID3树相同，且也是多了一个prune函数，并在fit函数最后调用prune函数。在Node类中，也有一个判断是否为叶子节点的函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8da841-0afb-4348-af81-22ea97275ace",
   "metadata": {},
   "source": [
    "**剪枝的CART树如下：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e842a9af-e830-4716-9ed1-e7fbd8fb2d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CART_Pruning:\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "\n",
    "    class Node:\n",
    "        def __init__(self, attribute=None, threshold=None, label=None, left=None, right=None):\n",
    "            self.attribute = attribute\n",
    "            self.threshold = threshold\n",
    "            self.label = label\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "\n",
    "        # 判断节点是否为叶子节点\n",
    "        def is_leaf(self):\n",
    "            return self.left is None and self.right is None\n",
    "\n",
    "    def prune(self, node):\n",
    "        # 如果当前节点有左右两个子节点\n",
    "        if node.left and node.right:\n",
    "            # 如果左右子节点都是叶子节点且它们的标签相同\n",
    "            if node.left.is_leaf() and node.right.is_leaf() and node.left.label == node.right.label:\n",
    "                # 将当前节点标签设为左子节点的标签，且删除左右子节点\n",
    "                node.label = node.left.label\n",
    "                node.left = None\n",
    "                node.right = None\n",
    "        else:\n",
    "            # 如果有左子节点，则递归对左子节点进行剪枝\n",
    "            if node.left:\n",
    "                self.prune(node.left)\n",
    "            # 如果有右子节点，则递归对右子节点进行剪枝\n",
    "            if node.right:\n",
    "                self.prune(node.right)\n",
    "        # 返回当前节点\n",
    "        return node\n",
    "\n",
    "    def fit(self, data, target):\n",
    "        # 使用 CART 决策树算法构建决策树，将根节点保存在 self.root 中\n",
    "        self.root = self._cart(data, target, list(range(len(data[0]))))\n",
    "        self.root = self.prune(self.root)  # 在训练结束后进行剪枝\n",
    "\n",
    "    def _cart(self, data, labels, attributes):\n",
    "        # 如果标签集唯一，创建叶子节点并返回\n",
    "        if len(set(labels)) == 1:\n",
    "            return self.Node(label=labels[0])\n",
    "\n",
    "        # 如果没有可用的属性，创建叶子节点并返回，标签为目标变量中最频繁的值\n",
    "        if len(attributes) == 0:\n",
    "            return self.Node(label=max(set(labels), key=labels.count))\n",
    "\n",
    "        # 寻找最佳分裂属性和阈值\n",
    "        best_attr, best_threshold = self._best_split(data, labels, attributes)\n",
    "        node = self.Node(attribute=best_attr, threshold=best_threshold)\n",
    "\n",
    "        # 根据最佳分裂属性和阈值分割数据集\n",
    "        left_data, left_labels, right_data, right_labels = self._split_data(data, labels, best_attr, best_threshold)\n",
    "\n",
    "        # 如果左子集为空，创建叶子节点并返回，标签为左子集中最频繁的值\n",
    "        if not left_data:\n",
    "            node.label = max(set(labels), key=labels.count)\n",
    "        else:\n",
    "            # 递归构建左子树和右子树\n",
    "            node.left = self._cart(left_data, left_labels, [attr for attr in attributes if attr != best_attr])\n",
    "            node.right = self._cart(right_data, right_labels, [attr for attr in attributes if attr != best_attr])\n",
    "\n",
    "        return node\n",
    "\n",
    "    def _best_split(self, data, labels, attributes):\n",
    "        # 初始化最佳分裂属性、最佳阈值和最小的基尼指数\n",
    "        best_attr, best_threshold, min_gini = None, None, float('inf')\n",
    "\n",
    "        # 遍历所有属性\n",
    "        for attr in attributes:\n",
    "            # 获取当前属性的所有取值\n",
    "            values = [row[attr] for row in data]\n",
    "\n",
    "            # 遍历当前属性的每个唯一取值，将数据集分割为左右两部分\n",
    "            for value in set(values):\n",
    "                left_labels = [l for v, l in zip(values, labels) if v < value]\n",
    "                right_labels = [l for v, l in zip(values, labels) if v >= value]\n",
    "\n",
    "                # 计算基尼指数\n",
    "                gini = self._gini(left_labels) * len(left_labels) / len(labels) + self._gini(right_labels) * len(\n",
    "                    right_labels) / len(labels)\n",
    "\n",
    "                # 更新最小基尼指数及相应的属性和阈值\n",
    "                if gini < min_gini:\n",
    "                    best_attr, best_threshold, min_gini = attr, value, gini\n",
    "\n",
    "        # 返回最佳分裂属性和阈值\n",
    "        return best_attr, best_threshold\n",
    "\n",
    "    def _split_data(self, data, labels, attr, threshold):\n",
    "        # 初始化左右两部分的数据和标签集\n",
    "        left_data, left_labels, right_data, right_labels = [], [], [], []\n",
    "\n",
    "        # 遍历数据集中的每一行及其对应的目标变量\n",
    "        for row, l in zip(data, labels):\n",
    "            # 根据指定的属性和阈值进行分割\n",
    "            if row[attr] < threshold:\n",
    "                # 如果当前样本的指定属性值小于阈值，加入左侧部分\n",
    "                left_data.append(row)\n",
    "                left_labels.append(l)\n",
    "            else:\n",
    "                # 如果当前样本的指定属性值大于等于阈值，加入右侧部分\n",
    "                right_data.append(row)\n",
    "                right_labels.append(l)\n",
    "\n",
    "        # 返回左右两部分的数据和标签集\n",
    "        return left_data, left_labels, right_data, right_labels\n",
    "\n",
    "    def _gini(self, labels):\n",
    "        # 初始化基尼指数\n",
    "        gini = 1 - sum((labels.count(value) / len(labels)) ** 2 for value in set(labels))\n",
    "        return gini\n",
    "\n",
    "    def predict(self, data):\n",
    "        # 返回一个列表，其中每个元素是对应样本的预测值\n",
    "        return [self._predict(row, self.root) for row in data]\n",
    "\n",
    "    def _predict(self, row, node):\n",
    "        # 如果当前节点是叶子节点，直接返回节点的标签作为预测值\n",
    "        if node.label is not None:\n",
    "            return node.label\n",
    "        # 如果当前节点不是叶子节点，根据节点的划分条件继续向下遍历\n",
    "        if row[node.attribute] < node.threshold:\n",
    "            # 如果样本的指定属性值小于节点的阈值，则递归预测左子树\n",
    "            return self._predict(row, node.left)\n",
    "        # 如果样本的指定属性值大于等于节点的阈值，则递归预测右子树\n",
    "        return self._predict(row, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d412de03-4cf3-4e41-97b2-4843be134080",
   "metadata": {},
   "source": [
    "**使用可以剪枝的CART决策树进行训练和预测：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "342a2290-18d9-4387-bd2b-b626f759f32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用可以剪枝的CART决策树进行训练和预测\n",
    "def CRAT_Pruning_train_predict():\n",
    "    # 读取训练集和测试集数据\n",
    "    train_data = read_dataset('Watermelon-train2.csv')\n",
    "    test_data = read_dataset('Watermelon-test2.csv')\n",
    "\n",
    "    # 创建 CART 决策树实例\n",
    "    cart_pruning = CART_Pruning()\n",
    "\n",
    "    # 使用训练集训练 CART 决策树\n",
    "    cart_pruning.fit([row[1:-1] for row in train_data], [row[-1] for row in train_data])\n",
    "\n",
    "    # 对测试集进行预测\n",
    "    predictions = cart_pruning.predict([row[1:-1] for row in test_data])\n",
    "\n",
    "    # 计算分类精度\n",
    "    accuracy = sum(p == t[-1] for p, t in zip(predictions, test_data)) / len(test_data)\n",
    "\n",
    "    # 打印分类精度\n",
    "    print(f'分类精度: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6f90ee9-6406-4d39-9fee-2af0743982c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类精度: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "CRAT_Pruning_train_predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9860614-8d8b-4d64-b8d8-ae3955c3bdea",
   "metadata": {},
   "source": [
    "从结果可以看出，分类精度与没有使用剪枝是一样的。分析原因如下：\n",
    "+   剪枝的目的是提高模型的泛化能力，减少过拟合。我认为，原本的决策树本身已经足够简单，剪枝操作并未剔除过拟合的部分，所以，并没有带来显著的性能改善。\n",
    "+   本次实验的数据集的数据量比较小，模型难以在小数据集上学习复杂的关系。而过拟合通常发生在模型过于复杂，相反在数据集较小的情况下，模型可能会出现欠拟合的情况。而剪枝主要是针对过拟合的情况，所以精确度并未出现提升。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
